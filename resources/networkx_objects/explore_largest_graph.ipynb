{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploring the Largest Japanese Lexical Network Graph\n",
        "\n",
        "This notebook explores the largest NetworkX graph pickle file: `extract_lexical_network_with_graph_japanese_full.pkl` (190MB)\n",
        "\n",
        "## Overview\n",
        "- **File**: `extract_lexical_network_with_graph_japanese_full.pkl`\n",
        "- **Size**: 190MB\n",
        "- **Type**: NetworkX Graph (likely MultiDiGraph based on other files)\n",
        "- **Content**: Japanese lexical network with synonyms, antonyms, and semantic relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kiwisolver in c:\\users\\bpera\\.conda\\envs\\novi\\lib\\site-packages (1.3.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\bpera\\.conda\\envs\\novi\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\bpera\\.conda\\envs\\novi\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\bpera\\.conda\\envs\\novi\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\bpera\\.conda\\envs\\novi\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\bpera\\.conda\\envs\\novi\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\bpera\\.conda\\envs\\novi\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install kiwisolver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.13.1)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'd:/My_apps/aiLanguageTutor/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pickle\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "from collections import Counter\n",
        "import time\n",
        "import os\n",
        "\n",
        "# # Set up plotting\n",
        "# plt.style.use('default')\n",
        "# sns.set_palette(\"husl\")\n",
        "\n",
        "# # Display options\n",
        "# pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.max_rows', 50)\n",
        "# pd.set_option('display.width', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. File Information and Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: extract_lexical_network_with_graph_japanese_full.pkl\n",
            "Size: 190.40 MB\n",
            "File exists: True\n"
          ]
        }
      ],
      "source": [
        "# Check file information\n",
        "file_path = 'extract_lexical_network_with_graph_japanese_full.pkl'\n",
        "file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "print(f\"File: {file_path}\")\n",
        "print(f\"Size: {file_size_mb:.2f} MB\")\n",
        "print(f\"File exists: {os.path.exists(file_path)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the large graph...\n",
            "Graph loaded successfully in 10.40 seconds\n"
          ]
        }
      ],
      "source": [
        "# Load the graph with progress tracking\n",
        "print(\"Loading the large graph...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'rb') as f:\n",
        "        G = pickle.load(f)\n",
        "    \n",
        "    load_time = time.time() - start_time\n",
        "    print(f\"Graph loaded successfully in {load_time:.2f} seconds\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading graph: {e}\")\n",
        "    G = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic Graph Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GRAPH BASIC INFORMATION ===\n",
            "Graph type: <class 'networkx.classes.multidigraph.MultiDiGraph'>\n",
            "Number of nodes: 264,306\n",
            "Number of edges: 1,132,834\n",
            "Is directed: True\n",
            "Is multigraph: True\n",
            "Graph density: 0.000016\n",
            "Number of weakly connected components: 96935\n",
            "Number of strongly connected components: 152187\n"
          ]
        }
      ],
      "source": [
        "if G is not None:\n",
        "    print(\"=== GRAPH BASIC INFORMATION ===\")\n",
        "    print(f\"Graph type: {type(G)}\")\n",
        "    print(f\"Number of nodes: {G.number_of_nodes():,}\")\n",
        "    print(f\"Number of edges: {G.number_of_edges():,}\")\n",
        "    print(f\"Is directed: {G.is_directed()}\")\n",
        "    print(f\"Is multigraph: {G.is_multigraph()}\")\n",
        "    print(f\"Graph density: {nx.density(G):.6f}\")\n",
        "    \n",
        "    # Check if graph is connected\n",
        "    if G.is_directed():\n",
        "        print(f\"Number of weakly connected components: {nx.number_weakly_connected_components(G)}\")\n",
        "        print(f\"Number of strongly connected components: {nx.number_strongly_connected_components(G)}\")\n",
        "    else:\n",
        "        print(f\"Number of connected components: {nx.number_connected_components(G)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Node Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== NODE ANALYSIS ===\n",
            "Sample nodes: ['物', '品', '物と品', '物品', '物体', '物と物体', '物理', '物と物品', '商業', '対象']\n",
            "\n",
            "Sample node attributes:\n",
            "物: {'kanji': '物', 'hiragana': 'もの', 'translation': 'thing'}\n",
            "品: {'kanji': '品', 'hiragana': 'しな', 'translation': 'item'}\n",
            "物と品: {'kanji': '物と品', 'hiragana': 'ものとしな', 'translation': 'thing and item'}\n",
            "物品: {'kanji': '物品', 'hiragana': 'ぶっぴん', 'translation': 'goods'}\n",
            "物体: {'kanji': '物体', 'hiragana': 'ぶったい', 'translation': 'body'}\n",
            "物と物体: {'kanji': '物と物体', 'hiragana': 'ものとぶったい', 'translation': 'thing and body'}\n",
            "物理: {'kanji': '物理', 'hiragana': 'ぶつり', 'translation': 'physics'}\n",
            "物と物品: {'kanji': '物と物品', 'hiragana': 'ものとぶっぴん', 'translation': 'thing and goods'}\n",
            "商業: {'kanji': '商業', 'hiragana': 'しょうぎょう', 'translation': 'commerce'}\n",
            "対象: {'kanji': '対象', 'hiragana': 'たいしょう', 'translation': 'target'}\n",
            "\n",
            "All node attribute keys: ['hiragana', 'kanji', 'translation']\n"
          ]
        }
      ],
      "source": [
        "if G is not None:\n",
        "    print(\"=== NODE ANALYSIS ===\")\n",
        "    \n",
        "    # Get sample nodes\n",
        "    sample_nodes = list(G.nodes())[:10]\n",
        "    print(f\"Sample nodes: {sample_nodes}\")\n",
        "    \n",
        "    # Analyze node attributes\n",
        "    node_attrs = {}\n",
        "    for node in sample_nodes:\n",
        "        node_attrs[node] = dict(G.nodes[node])\n",
        "    \n",
        "    print(\"\\nSample node attributes:\")\n",
        "    for node, attrs in node_attrs.items():\n",
        "        print(f\"{node}: {attrs}\")\n",
        "    \n",
        "    # Get all unique attribute keys\n",
        "    all_attr_keys = set()\n",
        "    for node in G.nodes():\n",
        "        all_attr_keys.update(G.nodes[node].keys())\n",
        "    \n",
        "    print(f\"\\nAll node attribute keys: {sorted(all_attr_keys)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DEGREE DISTRIBUTION ===\n",
            "Average degree: 8.57\n",
            "Median degree: 1.00\n",
            "Max degree: 4874\n",
            "Min degree: 0\n",
            "\n",
            "Top 20 nodes by degree:\n",
            "社会: 4874\n",
            "文化: 4864\n",
            "生物: 3307\n",
            "教育: 3179\n",
            "経済: 2742\n",
            "環境: 2485\n",
            "技術: 2388\n",
            "感情: 2213\n",
            "自然: 1988\n",
            "構造: 1931\n",
            "生態系: 1864\n",
            "情報: 1773\n",
            "法律: 1740\n",
            "文書: 1733\n",
            "組織: 1713\n",
            "芸術: 1711\n",
            "地域: 1703\n",
            "ビジネス: 1697\n",
            "表現: 1614\n",
            "地理: 1589\n"
          ]
        }
      ],
      "source": [
        "if G is not None:\n",
        "    # Degree distribution\n",
        "    degrees = [d for n, d in G.degree()]\n",
        "    \n",
        "    print(\"=== DEGREE DISTRIBUTION ===\")\n",
        "    print(f\"Average degree: {np.mean(degrees):.2f}\")\n",
        "    print(f\"Median degree: {np.median(degrees):.2f}\")\n",
        "    print(f\"Max degree: {max(degrees)}\")\n",
        "    print(f\"Min degree: {min(degrees)}\")\n",
        "    \n",
        "    # Top nodes by degree\n",
        "    top_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:20]\n",
        "    print(\"\\nTop 20 nodes by degree:\")\n",
        "    for node, degree in top_nodes:\n",
        "        print(f\"{node}: {degree}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Edge Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EDGE ANALYSIS ===\n",
            "Sample edges:\n",
            "物 -> 品: {'relation_type': 'SYNONYM', 'synonymity_strength': 0.9, 'relation_type_strength': 0.9, 'relation_explanation': 'Both words refer to objects or items, often used interchangeably in certain contexts.'}\n",
            "物 -> 物品: {'relation_type': 'HAS_DOMAIN', 'relation_type_strength': 0.8, 'relation_explanation': 'Both words belong to the category of physical objects or goods.'}\n",
            "物 -> 物品: {'relation_type': 'SYNONYM', 'synonymity_strength': 0.9, 'relation_type_strength': 0.9, 'relation_explanation': 'Both words refer to tangible items or products, often used in commerce.'}\n",
            "物 -> 物品: {'relation_type': 'HYPONYM', 'synonymity_strength': 0.7, 'relation_type_strength': 0.85, 'relation_explanation': '物品 refers to tangible items or goods, which are specific instances of 物.'}\n",
            "物 -> 物品: {'relation_type': 'CO-HYPONYM', 'synonymity_strength': 0.7, 'relation_type_strength': 0.8, 'relation_explanation': \"Both words refer to tangible items, with '物品' being a specific type of '物'.\"}\n",
            "物 -> 物品: {'relation_type': 'HOLONYM', 'synonymity_strength': 0.75, 'relation_type_strength': 0.85, 'relation_explanation': '物品 refers to a whole category of goods that are made up of various things.'}\n",
            "物 -> 物体: {'relation_type': 'SYNONYM', 'synonymity_strength': 0.85, 'relation_type_strength': 0.85, 'relation_explanation': \"Both terms refer to physical entities, with '物体' being a more specific term for a physical object.\"}\n",
            "物 -> 物体: {'relation_type': 'HYPONYM', 'synonymity_strength': 0.75, 'relation_type_strength': 0.9, 'relation_explanation': '物体 refers to a physical object, which is a specific instance of 物.'}\n",
            "物 -> 物体: {'relation_type': 'CO-HYPONYM', 'synonymity_strength': 0.7, 'relation_type_strength': 0.8, 'relation_explanation': \"Both words refer to physical entities, with '物体' being a specific type of '物'.\"}\n",
            "物 -> 物体: {'relation_type': 'HOLONYM', 'synonymity_strength': 0.7, 'relation_type_strength': 0.8, 'relation_explanation': '物体 refers to a whole that includes various physical things or bodies.'}\n",
            "\n",
            "All edge attribute keys: ['original', 'relation_explanation', 'relation_type', 'relation_type_strength', 'synonymity_strength']\n"
          ]
        }
      ],
      "source": [
        "if G is not None:\n",
        "    print(\"=== EDGE ANALYSIS ===\")\n",
        "    \n",
        "    # Get sample edges\n",
        "    sample_edges = list(G.edges(data=True))[:10]\n",
        "    print(\"Sample edges:\")\n",
        "    for u, v, data in sample_edges:\n",
        "        print(f\"{u} -> {v}: {data}\")\n",
        "    \n",
        "    # Analyze edge attributes\n",
        "    all_edge_attr_keys = set()\n",
        "    for u, v, data in G.edges(data=True):\n",
        "        all_edge_attr_keys.update(data.keys())\n",
        "    \n",
        "    print(f\"\\nAll edge attribute keys: {sorted(all_edge_attr_keys)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "if G is not None:\n",
        "    # Analyze edge types if 'relation' attribute exists\n",
        "    if 'relation' in all_edge_attr_keys:\n",
        "        edge_types = Counter()\n",
        "        for u, v, data in G.edges(data=True):\n",
        "            if 'relation' in data:\n",
        "                edge_types[data['relation']] += 1\n",
        "        \n",
        "        print(\"=== EDGE TYPES ===\")\n",
        "        for edge_type, count in edge_types.most_common():\n",
        "            print(f\"{edge_type}: {count:,}\")\n",
        "    \n",
        "    # Analyze edge types if 'type' attribute exists\n",
        "    if 'type' in all_edge_attr_keys:\n",
        "        edge_types = Counter()\n",
        "        for u, v, data in G.edges(data=True):\n",
        "            if 'type' in data:\n",
        "                edge_types[data['type']] += 1\n",
        "        \n",
        "        print(\"\\n=== EDGE TYPES (type attribute) ===\")\n",
        "        for edge_type, count in edge_types.most_common():\n",
        "            print(f\"{edge_type}: {count:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Graph Structure Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GRAPH STRUCTURE ANALYSIS ===\n",
            "Number of components: 96935\n",
            "Largest component size: 167,372\n",
            "Second largest component size: 1\n",
            "Average component size: 2.73\n",
            "\n",
            "Top 10 component sizes:\n",
            "1. 167,372\n",
            "2. 1\n",
            "3. 1\n",
            "4. 1\n",
            "5. 1\n",
            "6. 1\n",
            "7. 1\n",
            "8. 1\n",
            "9. 1\n",
            "10. 1\n"
          ]
        }
      ],
      "source": [
        "if G is not None:\n",
        "    print(\"=== GRAPH STRUCTURE ANALYSIS ===\")\n",
        "    \n",
        "    # Component analysis\n",
        "    if G.is_directed():\n",
        "        components = list(nx.weakly_connected_components(G))\n",
        "    else:\n",
        "        components = list(nx.connected_components(G))\n",
        "    \n",
        "    component_sizes = [len(c) for c in components]\n",
        "    component_sizes.sort(reverse=True)\n",
        "    \n",
        "    print(f\"Number of components: {len(components)}\")\n",
        "    print(f\"Largest component size: {component_sizes[0]:,}\")\n",
        "    print(f\"Second largest component size: {component_sizes[1] if len(component_sizes) > 1 else 0:,}\")\n",
        "    print(f\"Average component size: {np.mean(component_sizes):.2f}\")\n",
        "    \n",
        "    # Show top 10 component sizes\n",
        "    print(\"\\nTop 10 component sizes:\")\n",
        "    for i, size in enumerate(component_sizes[:10]):\n",
        "        print(f\"{i+1}. {size:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Japanese Language Specific Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== JAPANESE LANGUAGE ANALYSIS ===\n",
            "Sample analysis of first 1000 nodes:\n",
            "Kanji characters found: 447\n",
            "Hiragana characters found: 44\n",
            "Katakana characters found: 64\n",
            "\n",
            "Sample kanji characters: ['皆', '標', '治', '柄', '微', '離', '観', '住', '四', '資', '史', '自', '更', '取', '度', '日', '企', '複', '節', '繰']\n",
            "Sample hiragana characters: ['よ', 'び', 'け', 'く', 'り', 'わ', 'の', 'こ', 'と', 'ん', 'て', 'ら', 'あ', 'ゆ', 'も', 'む', 'に', 'れ', 'げ', 'を']\n",
            "Sample katakana characters: ['ハ', 'サ', 'ィ', 'デ', 'ロ', 'メ', 'テ', 'ム', 'チ', 'シ', 'ン', 'ビ', 'ナ', 'ッ', 'ケ', 'ズ', 'ポ', 'ツ', 'フ', 'ル']\n"
          ]
        }
      ],
      "source": [
        "if G is not None:\n",
        "    print(\"=== JAPANESE LANGUAGE ANALYSIS ===\")\n",
        "    \n",
        "    # Analyze Japanese characters in node names\n",
        "    japanese_chars = []\n",
        "    kanji_chars = []\n",
        "    hiragana_chars = []\n",
        "    katakana_chars = []\n",
        "    \n",
        "    for node in list(G.nodes())[:1000]:  # Sample first 1000 nodes\n",
        "        if isinstance(node, str):\n",
        "            for char in node:\n",
        "                if '\\u4e00' <= char <= '\\u9fff':  # Kanji\n",
        "                    kanji_chars.append(char)\n",
        "                elif '\\u3040' <= char <= '\\u309f':  # Hiragana\n",
        "                    hiragana_chars.append(char)\n",
        "                elif '\\u30a0' <= char <= '\\u30ff':  # Katakana\n",
        "                    katakana_chars.append(char)\n",
        "    \n",
        "    print(f\"Sample analysis of first 1000 nodes:\")\n",
        "    print(f\"Kanji characters found: {len(set(kanji_chars))}\")\n",
        "    print(f\"Hiragana characters found: {len(set(hiragana_chars))}\")\n",
        "    print(f\"Katakana characters found: {len(set(katakana_chars))}\")\n",
        "    \n",
        "    # Show some examples\n",
        "    print(f\"\\nSample kanji characters: {list(set(kanji_chars))[:20]}\")\n",
        "    print(f\"Sample hiragana characters: {list(set(hiragana_chars))[:20]}\")\n",
        "    print(f\"Sample katakana characters: {list(set(katakana_chars))[:20]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Memory Usage and Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MEMORY AND PERFORMANCE ANALYSIS ===\n",
            "Graph object size: 0.00 MB\n",
            "Estimated nodes memory: 20.11 MB\n",
            "Estimated edges memory: 60.50 MB\n",
            "Total estimated memory: 80.61 MB\n"
          ]
        }
      ],
      "source": [
        "if G is not None:\n",
        "    print(\"=== MEMORY AND PERFORMANCE ANALYSIS ===\")\n",
        "    \n",
        "    # Estimate memory usage\n",
        "    import sys\n",
        "    graph_size = sys.getsizeof(G)\n",
        "    \n",
        "    # Rough estimation of node and edge memory\n",
        "    node_memory = sum(sys.getsizeof(node) for node in list(G.nodes())[:100]) * G.number_of_nodes() / 100\n",
        "    edge_memory = sum(sys.getsizeof(edge) for edge in list(G.edges())[:100]) * G.number_of_edges() / 100\n",
        "    \n",
        "    print(f\"Graph object size: {graph_size / (1024*1024):.2f} MB\")\n",
        "    print(f\"Estimated nodes memory: {node_memory / (1024*1024):.2f} MB\")\n",
        "    print(f\"Estimated edges memory: {edge_memory / (1024*1024):.2f} MB\")\n",
        "    print(f\"Total estimated memory: {(graph_size + node_memory + edge_memory) / (1024*1024):.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SUMMARY ===\n",
            "✅ Successfully loaded graph with 264,306 nodes and 1,132,834 edges\n",
            "✅ Graph type: MultiDiGraph\n",
            "✅ File size: 190.40 MB\n",
            "\n",
            "=== RECOMMENDATIONS ===\n",
            "1. For visualization: Use subgraphs or sample the graph\n",
            "2. For analysis: Consider using NetworkX's built-in algorithms\n",
            "3. For memory efficiency: Load only when needed\n",
            "4. For performance: Use parallel processing for large computations\n",
            "\n",
            "=== NEXT STEPS ===\n",
            "1. Create visualizations of subgraphs\n",
            "2. Analyze specific Japanese lexical relationships\n",
            "3. Export specific subgraphs for further analysis\n",
            "4. Compare with other graph versions in the project\n"
          ]
        }
      ],
      "source": [
        "if G is not None:\n",
        "    print(\"=== SUMMARY ===\")\n",
        "    print(f\"✅ Successfully loaded graph with {G.number_of_nodes():,} nodes and {G.number_of_edges():,} edges\")\n",
        "    print(f\"✅ Graph type: {type(G).__name__}\")\n",
        "    print(f\"✅ File size: {file_size_mb:.2f} MB\")\n",
        "    \n",
        "    print(\"\\n=== RECOMMENDATIONS ===\")\n",
        "    print(\"1. For visualization: Use subgraphs or sample the graph\")\n",
        "    print(\"2. For analysis: Consider using NetworkX's built-in algorithms\")\n",
        "    print(\"3. For memory efficiency: Load only when needed\")\n",
        "    print(\"4. For performance: Use parallel processing for large computations\")\n",
        "    \n",
        "    print(\"\\n=== NEXT STEPS ===\")\n",
        "    print(\"1. Create visualizations of subgraphs\")\n",
        "    print(\"2. Analyze specific Japanese lexical relationships\")\n",
        "    print(\"3. Export specific subgraphs for further analysis\")\n",
        "    print(\"4. Compare with other graph versions in the project\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
