# Legacy System Migration Plan - Japanese Lexical Graph

## ğŸ¯ Overview

This document outlines the comprehensive migration plan for the existing Japanese Lexical Graph system from [https://github.com/bperak/japanese-lexical-graph](https://github.com/bperak/japanese-lexical-graph) into our new AI Language Tutor architecture.

---

## ğŸ“Š **Legacy System Analysis**

### **Current Architecture (Legacy)**
```
Legacy System Components:
â”œâ”€â”€ Backend: Flask + NetworkX + Python
â”œâ”€â”€ Frontend: Three.js 3D visualization + HTML/CSS/JS
â”œâ”€â”€ Data: NetworkX Graph (G_synonyms_2024_09_18.pickle)
â”œâ”€â”€ AI Integration: Google Gemini API
â”œâ”€â”€ External APIs: Wikidata SPARQL integration
â”œâ”€â”€ Analysis: jreadability library for Japanese text
â”œâ”€â”€ Caching: Redis + in-memory caching
â””â”€â”€ Deployment: Gunicorn + systemd service
```

### **Key Features in Legacy System**
1. **3D Graph Visualization**: Interactive Three.js-based network visualization
2. **Japanese Lexical Network**: NetworkX graph of Japanese synonyms and relationships
3. **Wikidata Integration**: SPARQL queries for structured linguistic data
4. **AI-Powered Analysis**: Gemini API for explanations and term comparisons
5. **Readability Analysis**: jreadability library for Japanese text difficulty assessment
6. **Interactive Learning**: AI-generated exercises with difficulty validation
7. **Multi-Model Support**: Configurable Gemini model selection
8. **Caching System**: Performance optimization with Redis

### **Legacy Data Structure**
```python
# NetworkX Graph Structure (from pickle file)
G_synonyms = {
    'nodes': {
        'node_id': {
            'japanese_term': str,
            'english_meaning': str,
            'part_of_speech': str,
            'relationship_strength': float,
            'wikidata_id': str (optional),
            'readability_level': str (optional)
        }
    },
    'edges': {
        ('node1', 'node2'): {
            'relationship_type': 'synonym',
            'strength': float,
            'confidence': float
        }
    }
}
```

---

## ğŸš€ **Migration Strategy**

### **Phase 1: Data Migration (NetworkX â†’ Neo4j)**

#### **1.1 Data Extraction & Analysis**
```python
# Legacy data extraction script
class LegacyDataExtractor:
    def __init__(self, pickle_file_path: str):
        self.legacy_graph = self.load_networkx_graph(pickle_file_path)
        
    def load_networkx_graph(self, file_path: str) -> nx.Graph:
        """Load the existing NetworkX graph from pickle file"""
        with open(file_path, 'rb') as f:
            return pickle.load(f)
    
    def analyze_graph_structure(self) -> GraphAnalysis:
        """Analyze the structure and content of legacy graph"""
        return {
            "total_nodes": len(self.legacy_graph.nodes()),
            "total_edges": len(self.legacy_graph.edges()),
            "node_attributes": list(self.legacy_graph.nodes(data=True)[0][1].keys()),
            "edge_attributes": list(self.legacy_graph.edges(data=True)[0][2].keys()),
            "relationship_types": self.get_relationship_types(),
            "data_quality_assessment": self.assess_data_quality()
        }
    
    def extract_structured_data(self) -> Dict[str, List]:
        """Extract data in format suitable for Neo4j migration"""
        nodes_data = []
        relationships_data = []
        
        # Extract nodes
        for node_id, attributes in self.legacy_graph.nodes(data=True):
            nodes_data.append({
                "id": node_id,
                "japanese_term": attributes.get('japanese_term'),
                "english_meaning": attributes.get('english_meaning'),
                "part_of_speech": attributes.get('part_of_speech'),
                "wikidata_id": attributes.get('wikidata_id'),
                "readability_level": attributes.get('readability_level'),
                "source": "legacy_migration",
                "migration_date": datetime.now().isoformat()
            })
        
        # Extract relationships
        for source, target, attributes in self.legacy_graph.edges(data=True):
            relationships_data.append({
                "source_id": source,
                "target_id": target,
                "relationship_type": attributes.get('relationship_type', 'synonym'),
                "strength": attributes.get('strength', 0.5),
                "confidence": attributes.get('confidence', 0.5),
                "source": "legacy_migration"
            })
        
        return {
            "nodes": nodes_data,
            "relationships": relationships_data
        }
```

#### **1.2 Neo4j Schema Design for Legacy Data**
```cypher
-- Enhanced Neo4j schema to accommodate legacy data
CREATE CONSTRAINT word_id_unique IF NOT EXISTS FOR (w:Word) REQUIRE w.id IS UNIQUE;
CREATE CONSTRAINT grammar_point_id_unique IF NOT EXISTS FOR (g:GrammarPoint) REQUIRE g.id IS UNIQUE;

-- Legacy-specific indexes
CREATE INDEX word_japanese_term IF NOT EXISTS FOR (w:Word) ON (w.japanese_term);
CREATE INDEX word_english_meaning IF NOT EXISTS FOR (w:Word) ON (w.english_meaning);
CREATE INDEX word_part_of_speech IF NOT EXISTS FOR (w:Word) ON (w.part_of_speech);
CREATE INDEX word_readability_level IF NOT EXISTS FOR (w:Word) ON (w.readability_level);

-- Enhanced Word node properties to include legacy data
// Word node with legacy migration support
(:Word {
  id: string,
  japanese_term: string,
  english_meaning: string,
  part_of_speech: string,
  readability_level: string,
  wikidata_id: string,
  source: "legacy_migration",
  migration_date: datetime,
  legacy_node_id: string,
  status: "migrated"
})

-- Legacy relationship types
(:Word)-[:SYNONYM {strength: float, confidence: float, source: "legacy"}]->(:Word)
(:Word)-[:RELATED_TO {strength: float, confidence: float, source: "legacy"}]->(:Word)
```

#### **1.3 Migration Script Implementation**
```python
# Neo4j migration script
class Neo4jMigrationService:
    def __init__(self, neo4j_driver, legacy_extractor):
        self.driver = neo4j_driver
        self.legacy_data = legacy_extractor.extract_structured_data()
    
    def migrate_nodes(self):
        """Migrate all nodes from legacy NetworkX to Neo4j"""
        with self.driver.session() as session:
            for node_data in self.legacy_data["nodes"]:
                session.run("""
                    MERGE (w:Word {id: $id})
                    SET w.japanese_term = $japanese_term,
                        w.english_meaning = $english_meaning,
                        w.part_of_speech = $part_of_speech,
                        w.readability_level = $readability_level,
                        w.wikidata_id = $wikidata_id,
                        w.source = $source,
                        w.migration_date = datetime($migration_date),
                        w.status = 'migrated'
                """, **node_data)
    
    def migrate_relationships(self):
        """Migrate all relationships from legacy NetworkX to Neo4j"""
        with self.driver.session() as session:
            for rel_data in self.legacy_data["relationships"]:
                session.run("""
                    MATCH (source:Word {id: $source_id})
                    MATCH (target:Word {id: $target_id})
                    MERGE (source)-[r:SYNONYM]->(target)
                    SET r.strength = $strength,
                        r.confidence = $confidence,
                        r.source = $source,
                        r.migration_date = datetime()
                """, **rel_data)
    
    def validate_migration(self) -> MigrationValidation:
        """Validate the migration was successful"""
        with self.driver.session() as session:
            # Count migrated nodes and relationships
            node_count = session.run("MATCH (w:Word {source: 'legacy_migration'}) RETURN count(w)").single()[0]
            rel_count = session.run("MATCH ()-[r {source: 'legacy'}]-() RETURN count(r)").single()[0]
            
            return {
                "migrated_nodes": node_count,
                "migrated_relationships": rel_count,
                "expected_nodes": len(self.legacy_data["nodes"]),
                "expected_relationships": len(self.legacy_data["relationships"]),
                "migration_success": node_count == len(self.legacy_data["nodes"]) and 
                                   rel_count == len(self.legacy_data["relationships"])
            }
```

### **Phase 2: Feature Integration**

#### **2.1 3D Visualization Integration**
```javascript
// Enhanced 3D visualization for new architecture
class Enhanced3DVisualization {
    constructor(containerId, neo4jEndpoint) {
        this.container = document.getElementById(containerId);
        this.neo4jEndpoint = neo4jEndpoint;
        this.scene = new THREE.Scene();
        this.camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        this.renderer = new THREE.WebGLRenderer();
        this.forceGraph = new ForceGraph3D(this.container);
        
        this.initializeVisualization();
    }
    
    async loadGraphData(query = null) {
        // Fetch data from Neo4j via FastAPI endpoint
        const response = await fetch(`${this.neo4jEndpoint}/api/v1/graph/visualization`, {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({
                query: query,
                include_legacy: true,
                max_nodes: 1000
            })
        });
        
        const graphData = await response.json();
        this.renderGraph(graphData);
    }
    
    renderGraph(graphData) {
        this.forceGraph
            .graphData(graphData)
            .nodeLabel('japanese_term')
            .nodeColor(node => this.getNodeColor(node))
            .linkColor(() => 'rgba(255,255,255,0.2)')
            .onNodeClick(node => this.handleNodeClick(node))
            .onNodeHover(node => this.handleNodeHover(node));
    }
    
    getNodeColor(node) {
        // Color coding based on data source and type
        if (node.source === 'legacy_migration') return '#FF6B6B';  // Red for legacy
        if (node.type === 'GrammarPoint') return '#4ECDC4';        // Teal for grammar
        if (node.type === 'Word') return '#45B7D1';               // Blue for words
        return '#96CEB4';  // Default green
    }
    
    async handleNodeClick(node) {
        // Enhanced node interaction with new API
        const response = await fetch(`${this.neo4jEndpoint}/api/v1/knowledge/${node.id}/details`);
        const nodeDetails = await response.json();
        
        this.displayNodeDetails(nodeDetails);
        this.highlightConnections(node.id);
    }
}
```

#### **2.2 Wikidata Integration Enhancement**
```python
# Enhanced Wikidata service for new architecture
class WikidataIntegrationService:
    def __init__(self):
        self.sparql = SPARQLWrapper("https://query.wikidata.org/sparql")
        self.cache = {}
    
    async def enrich_node_with_wikidata(self, node_id: str) -> WikidataEnrichment:
        """Enrich Neo4j node with Wikidata information"""
        # Get node from Neo4j
        node_data = await self.get_node_from_neo4j(node_id)
        
        if not node_data.get('wikidata_id'):
            # Try to find Wikidata ID if not present
            wikidata_id = await self.find_wikidata_id(node_data['japanese_term'])
            if wikidata_id:
                await self.update_node_wikidata_id(node_id, wikidata_id)
        
        # Fetch comprehensive Wikidata information
        wikidata_info = await self.fetch_wikidata_details(node_data.get('wikidata_id'))
        
        return {
            "node_id": node_id,
            "wikidata_info": wikidata_info,
            "enrichment_date": datetime.now().isoformat(),
            "data_quality_score": self.calculate_data_quality(wikidata_info)
        }
    
    async def fetch_wikidata_details(self, wikidata_id: str) -> Dict:
        """Fetch comprehensive Wikidata information"""
        query = f"""
        SELECT ?item ?itemLabel ?description ?pronunciation ?etymology ?usage ?synonyms WHERE {{
            VALUES ?item {{ wd:{wikidata_id} }}
            OPTIONAL {{ ?item schema:description ?description . FILTER(LANG(?description) = "en") }}
            OPTIONAL {{ ?item wdt:P443 ?pronunciation }}
            OPTIONAL {{ ?item wdt:P5191 ?etymology }}
            OPTIONAL {{ ?item wdt:P2354 ?usage }}
            OPTIONAL {{ ?item wdt:P5973 ?synonyms }}
            SERVICE wikibase:label {{ bd:serviceParam wikibase:language "ja,en" }}
        }}
        """
        
        return await self.execute_sparql_query(query)
```

#### **2.3 AI Analysis Integration**
```python
# Enhanced AI analysis service combining legacy and new features
class EnhancedAIAnalysisService:
    def __init__(self, openai_client, gemini_client):
        self.openai_client = openai_client
        self.gemini_client = gemini_client
        self.readability_analyzer = JapaneseReadabilityAnalyzer()
    
    async def comprehensive_term_analysis(self, term_id: str) -> ComprehensiveAnalysis:
        """Comprehensive analysis combining multiple AI providers and legacy features"""
        
        # Get term data from Neo4j
        term_data = await self.get_term_from_neo4j(term_id)
        
        # Parallel analysis with multiple providers
        analyses = await asyncio.gather(
            self.gemini_linguistic_analysis(term_data),
            self.openai_cultural_analysis(term_data),
            self.readability_analysis(term_data),
            self.wikidata_context_analysis(term_data),
            self.generate_learning_exercises(term_data)
        )
        
        return {
            "term_id": term_id,
            "linguistic_analysis": analyses[0],
            "cultural_analysis": analyses[1],
            "readability_assessment": analyses[2],
            "wikidata_context": analyses[3],
            "learning_exercises": analyses[4],
            "comprehensive_score": self.calculate_comprehensive_score(analyses),
            "analysis_timestamp": datetime.now().isoformat()
        }
    
    async def gemini_linguistic_analysis(self, term_data: Dict) -> LinguisticAnalysis:
        """Legacy Gemini analysis enhanced for new architecture"""
        prompt = f"""
        Analyze the Japanese term: {term_data['japanese_term']}
        English meaning: {term_data['english_meaning']}
        Part of speech: {term_data['part_of_speech']}
        
        Provide comprehensive linguistic analysis including:
        1. Morphological breakdown
        2. Semantic relationships
        3. Usage patterns and contexts
        4. Cultural nuances
        5. Learning difficulty assessment
        6. Common mistakes learners make
        """
        
        response = await self.gemini_client.generate_content(prompt)
        return self.parse_linguistic_analysis(response.text)
```

### **Phase 3: Architecture Integration**

#### **3.1 FastAPI Endpoints for Legacy Features**
```python
# FastAPI endpoints integrating legacy functionality
@app.get("/api/v1/graph/legacy-visualization")
async def get_legacy_visualization_data(
    query: Optional[str] = None,
    max_nodes: int = 1000,
    include_wikidata: bool = True
):
    """Get graph data optimized for 3D visualization (legacy feature)"""
    
    # Enhanced Cypher query for visualization
    cypher_query = """
    MATCH (n:Word)
    WHERE ($query IS NULL OR n.japanese_term CONTAINS $query OR n.english_meaning CONTAINS $query)
    WITH n LIMIT $max_nodes
    OPTIONAL MATCH (n)-[r]-(connected)
    RETURN n, r, connected
    """
    
    # Execute query and format for Three.js
    results = await neo4j_service.execute_query(cypher_query, {
        "query": query,
        "max_nodes": max_nodes
    })
    
    # Format data for 3D visualization
    graph_data = format_for_threejs(results, include_wikidata)
    
    return graph_data

@app.get("/api/v1/analysis/comprehensive/{term_id}")
async def get_comprehensive_analysis(term_id: str, model_name: Optional[str] = None):
    """Comprehensive term analysis combining all legacy and new features"""
    
    analysis_service = EnhancedAIAnalysisService(openai_client, gemini_client)
    
    # Use specified model or default
    if model_name:
        analysis_service.set_preferred_model(model_name)
    
    comprehensive_analysis = await analysis_service.comprehensive_term_analysis(term_id)
    
    return comprehensive_analysis

@app.post("/api/v1/comparison/terms")
async def compare_terms(comparison_request: TermComparisonRequest):
    """Enhanced term comparison (legacy feature) with new AI capabilities"""
    
    comparison_service = TermComparisonService()
    
    comparison_result = await comparison_service.compare_terms(
        term1_id=comparison_request.term1_id,
        term2_id=comparison_request.term2_id,
        analysis_depth=comparison_request.analysis_depth,
        include_exercises=comparison_request.include_exercises
    )
    
    return comparison_result
```

#### **3.2 Enhanced Human Tutor Interface for Legacy Data**
```streamlit
# Enhanced validation interface for legacy data
st.title("ğŸ”„ Legacy Data Management & Enhancement")

# Legacy data overview
st.subheader("ğŸ“Š Legacy System Integration Status")

col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Migrated Nodes", "12,847", "+100%")
with col2:
    st.metric("Migrated Relationships", "45,231", "+100%")
with col3:
    st.metric("Wikidata Enriched", "8,942", "69.6%")
with col4:
    st.metric("AI Enhanced", "6,123", "47.7%")

# Legacy data quality assessment
st.subheader("ğŸ” Data Quality Assessment")

# Show data quality metrics
quality_metrics = get_legacy_data_quality_metrics()
display_data_quality_dashboard(quality_metrics)

# Enhancement workflow
st.subheader("âš¡ Legacy Data Enhancement")

# Select legacy nodes for enhancement
legacy_nodes = get_unenhanced_legacy_nodes()
selected_nodes = st.multiselect("Select nodes for AI enhancement:", legacy_nodes)

if st.button("ğŸ¤– Enhance Selected Nodes"):
    with st.spinner("Enhancing nodes with AI analysis..."):
        enhancement_results = enhance_legacy_nodes_with_ai(selected_nodes)
        st.success(f"Enhanced {len(selected_nodes)} nodes successfully!")
        st.json(enhancement_results)

# Wikidata enrichment workflow
st.subheader("ğŸŒ Wikidata Enrichment")

# Find nodes missing Wikidata IDs
nodes_missing_wikidata = get_nodes_missing_wikidata()
st.write(f"Found {len(nodes_missing_wikidata)} nodes without Wikidata IDs")

if st.button("ğŸ” Auto-Find Wikidata IDs"):
    with st.spinner("Searching Wikidata for matching entries..."):
        wikidata_matches = auto_find_wikidata_matches(nodes_missing_wikidata)
        st.success(f"Found {len(wikidata_matches)} potential matches!")
        
        # Show matches for human validation
        for match in wikidata_matches:
            display_wikidata_match_validation(match)
```

### **Phase 4: Testing & Validation**

#### **4.1 Migration Validation Tests**
```python
# Comprehensive migration testing
class MigrationValidationTests:
    def __init__(self, legacy_graph_path: str, neo4j_driver):
        self.legacy_graph = pickle.load(open(legacy_graph_path, 'rb'))
        self.neo4j_driver = neo4j_driver
    
    def test_node_count_consistency(self):
        """Verify all nodes migrated correctly"""
        legacy_count = len(self.legacy_graph.nodes())
        
        with self.neo4j_driver.session() as session:
            neo4j_count = session.run(
                "MATCH (n:Word {source: 'legacy_migration'}) RETURN count(n)"
            ).single()[0]
        
        assert legacy_count == neo4j_count, f"Node count mismatch: {legacy_count} vs {neo4j_count}"
    
    def test_relationship_integrity(self):
        """Verify all relationships migrated with correct properties"""
        legacy_edges = list(self.legacy_graph.edges(data=True))
        
        with self.neo4j_driver.session() as session:
            for source, target, data in legacy_edges:
                result = session.run("""
                    MATCH (s:Word {id: $source})-[r]-(t:Word {id: $target})
                    RETURN r.strength, r.confidence
                """, source=source, target=target)
                
                record = result.single()
                assert record, f"Missing relationship: {source} -> {target}"
                assert abs(record['r.strength'] - data.get('strength', 0.5)) < 0.001
    
    def test_data_quality_preservation(self):
        """Verify data quality is preserved during migration"""
        # Test Japanese character encoding
        # Test English translation accuracy
        # Test part-of-speech consistency
        # Test readability level preservation
        pass
    
    def test_feature_compatibility(self):
        """Verify legacy features work with migrated data"""
        # Test 3D visualization data format
        # Test Wikidata integration
        # Test AI analysis capabilities
        # Test search functionality
        pass
```

---

## ğŸ“‹ **Implementation Roadmap**

### **Phase 1: Data Migration (Weeks 1-2)**
- [ ] Analyze legacy NetworkX graph structure and data quality
- [ ] Design enhanced Neo4j schema to accommodate legacy data
- [ ] Implement data extraction and transformation scripts
- [ ] Execute migration with comprehensive validation
- [ ] Create data quality assessment and enhancement workflows

### **Phase 2: Feature Integration (Weeks 3-4)**
- [ ] Integrate 3D visualization with new FastAPI backend
- [ ] Enhance Wikidata integration for Neo4j architecture
- [ ] Upgrade AI analysis with multi-provider support
- [ ] Implement readability analysis in new system
- [ ] Create comprehensive term comparison features

### **Phase 3: UI Enhancement (Weeks 5-6)**
- [ ] Enhance human tutor interface for legacy data management
- [ ] Create migration monitoring and quality dashboards
- [ ] Implement batch enhancement workflows
- [ ] Add legacy data visualization and exploration tools

### **Phase 4: Testing & Optimization (Weeks 7-8)**
- [ ] Comprehensive migration validation testing
- [ ] Performance optimization for large graph datasets
- [ ] User acceptance testing with legacy features
- [ ] Documentation and training materials

---

## ğŸ¯ **Expected Benefits**

### **Data Preservation & Enhancement**
- **100% data preservation** from legacy system
- **Enhanced data quality** through AI-powered analysis
- **Improved performance** with Neo4j graph database
- **Better scalability** for future growth

### **Feature Enhancement**
- **Maintained 3D visualization** with improved performance
- **Enhanced AI analysis** with multi-provider support
- **Better Wikidata integration** with comprehensive caching
- **Improved readability analysis** with educational alignment

### **User Experience**
- **Seamless transition** for existing users
- **Enhanced functionality** with new AI capabilities
- **Better performance** and reliability
- **Professional tools** for content management

This comprehensive migration plan ensures that all valuable features and data from the legacy system are preserved and enhanced in the new AI Language Tutor architecture, while providing a smooth transition path and improved capabilities.